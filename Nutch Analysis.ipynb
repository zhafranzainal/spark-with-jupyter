{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dbfd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "!pip install --upgrade google-cloud-bigtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca0090d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------------+-----------------+--------------------+------------+--------------------+----------+-------------+--------------------+\n",
      "|           fetchTime|            metadata|        modifiedTime|retriesSinceFetch|retryIntervalDays|retryIntervalSeconds|       score|           signature|statusCode|   statusName|                 url|\n",
      "+--------------------+--------------------+--------------------+-----------------+-----------------+--------------------+------------+--------------------+----------+-------------+--------------------+\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|4.0064103E-4|                null|         1| db_unfetched|  http://www.3u.com/|\n",
      "|Wed Dec 13 21:00:...|{text/html, temp_...|Mon Nov 13 21:00:...|                0|               30|             2592000| 0.106155306|                null|         4|db_redir_temp|   https://amanz.me/|\n",
      "|Mon Nov 13 21:20:...|{text/html, temp_...|Thu Jan 01 07:30:...|                0|               30|             2592000|         0.0|                null|         1| db_unfetched|https://amanz.me/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|0.0030776516|                null|         1| db_unfetched|https://amanz.me/...|\n",
      "|Wed Dec 13 20:33:...|{text/html, succe...|Mon Nov 13 20:33:...|                0|               30|             2592000|   1.1030777|b2b285ab96f37c27f...|         2|   db_fetched|   https://amanz.my/|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|0.0016666667|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|0.0033333334|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|0.0016666667|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|0.0016666667|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|0.0033333334|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|0.0016666667|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|0.0016666667|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000| 0.010000001|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000| 0.006666667|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|0.0033333334|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|       0.005|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|0.0016666667|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|0.0016666667|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Mon Nov 13 21:20:...|{NULL, NULL, NULL...|Thu Jan 01 07:30:...|                0|               30|             2592000|0.0033333334|                null|         1| db_unfetched|https://amanz.my/...|\n",
      "|Wed Dec 13 21:00:...|{text/html, succe...|Mon Nov 13 21:00:...|                0|               30|             2592000|        0.05|f02868bdbf8821435...|         2|   db_fetched|https://amanz.my/...|\n",
      "+--------------------+--------------------+--------------------+-----------------+-----------------+--------------------+------------+--------------------+----------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Web Content Indexing\").getOrCreate()\n",
    "json_df = spark.read.json(\"part-r-00000\")\n",
    "json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc51475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "json_df = json_df.dropDuplicates().na.drop()\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"url\", outputCol=\"keywords\")\n",
    "json_df = tokenizer.transform(json_df)\n",
    "\n",
    "windowSpec = Window.orderBy(col(\"score\").desc())\n",
    "ranked_json_df = json_df.withColumn(\"rank\", rank().over(windowSpec))\n",
    "\n",
    "indexed_json_df = ranked_json_df.select(\"url\", \"keywords\", \"rank\")\n",
    "indexed_json_df.write.parquet(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df = spark.read.parquet(\"output/part-00000-7170e01f-c3fb-47a4-b6c9-9e7cce45d50a-c000.snappy.parquet\")\n",
    "parquet_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ddcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigtable\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "project_id = \"web-content-indexing\"\n",
    "instance_id = \"data-engineering-project\"\n",
    "table_id = \"crawled-links\"\n",
    "parquet_file = \"output/part-00000-7170e01f-c3fb-47a4-b6c9-9e7cce45d50a-c000.snappy.parquet\"\n",
    "\n",
    "# Set the path to your service account JSON key file\n",
    "credentials_path = \"web-content-indexing-78a6a13bc8de.json\"\n",
    "\n",
    "# Set the GOOGLE_APPLICATION_CREDENTIALS environment variable\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n",
    "\n",
    "# Create a Cloud Bigtable client using the service account credentials\n",
    "client = bigtable.Client(project=project_id,\n",
    "                         credentials=service_account.Credentials.from_service_account_file(credentials_path))\n",
    "\n",
    "# Connect to an existing Cloud Bigtable instance\n",
    "instance = client.instance(instance_id)\n",
    "\n",
    "# Open an existing table or create a new one\n",
    "table = instance.table(table_id)\n",
    "\n",
    "# Read the Parquet file into a Spark DataFrame\n",
    "parquet_df = spark.read.parquet(parquet_file)\n",
    "\n",
    "# Convert the DataFrame to a Pandas DataFrame for easy iteration\n",
    "pandas_df = parquet_df.toPandas()\n",
    "\n",
    "# Iterate over rows and write to Bigtable\n",
    "for index, row in pandas_df.iterrows():\n",
    "    row_key = str(row['url'])\n",
    "    column_family_id = \"cf1\"\n",
    "    column_id = \"c1\"\n",
    "\n",
    "    # Extract 'keywords' column value from the Pandas DataFrame\n",
    "    keywords_value = str(row['keywords'])\n",
    "\n",
    "    # Create a new row\n",
    "    row_key_bytes = row_key.encode(\"utf-8\")\n",
    "    row = table.row(row_key_bytes)\n",
    "\n",
    "    # Add the column value to the row\n",
    "    row.set_cell(column_family_id, column_id, keywords_value.encode(\"utf-8\"))  # Encode keywords_value to bytes\n",
    "\n",
    "    # Apply the mutation to the table\n",
    "    table.mutate_rows([row])\n",
    "\n",
    "    print(f\"Row with key '{row_key}' written to Bigtable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18bf54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigtable\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.bigtable.row_set import RowSet\n",
    "\n",
    "project_id = \"web-content-indexing\"\n",
    "instance_id = \"data-engineering-project\"\n",
    "table_id = \"crawled-links\"\n",
    "\n",
    "# Set the path to your service account JSON key file\n",
    "credentials_path = \"web-content-indexing-78a6a13bc8de.json\"\n",
    "\n",
    "# Set the GOOGLE_APPLICATION_CREDENTIALS environment variable\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n",
    "\n",
    "# Create a Cloud Bigtable client using the service account credentials\n",
    "client = bigtable.Client(project=project_id,\n",
    "                         credentials=service_account.Credentials.from_service_account_file(credentials_path))\n",
    "\n",
    "# Connect to an existing Cloud Bigtable instance\n",
    "instance = client.instance(instance_id)\n",
    "\n",
    "# Open an existing table\n",
    "table = instance.table(table_id)\n",
    "\n",
    "# Read data from Bigtable\n",
    "rows = table.read_rows()\n",
    "\n",
    "# Create an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate over rows and extract data\n",
    "for row in rows:\n",
    "    # Extract row key\n",
    "    row_key = row.row_key.decode(\"utf-8\")\n",
    "\n",
    "    # Extract column family and column values\n",
    "    columns_data = {}\n",
    "    for family, columns in row.cells.items():\n",
    "        for column, cells in columns.items():\n",
    "            value = cells[0].value.decode(\"utf-8\")\n",
    "            columns_data[f\"{family}:{column}\"] = value\n",
    "\n",
    "    # Create a dictionary representing the row\n",
    "    row_data = {\"RowKey\": row_key, **columns_data}\n",
    "\n",
    "    # Append the row data to the list\n",
    "    data.append(row_data)\n",
    "\n",
    "# Create a Pandas DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame in Jupyter\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57893b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
